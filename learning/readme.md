# Primary Lessons Learned about Reinforcement Learning during this Project
- **Work on a good state representation** - originally, I was representing the entire board as 0's and 1's. I figured the model could eventually gain an idea for how the board works. Well, maybe it would. But this wasnt the most efficient representation. Instead, I developed a "relative depth" representation that would only describe the available drop depth at any given point in the game; in other words, it represented the difference in levels between all columns. This was *much* simpler and allowed the model to learn much more quickly!-
- **Develop a good reward model** - develop a nice reward model that incentivizes positive gameplay. For example, the final score of the game I found was not enough for a strong learning speed. Once I added "bonuses" for having all columns of a row occupied, it learned much quicker.
- **Force exploration with e-greedy** - the model found local optimums very early on and would stick to them, doing weird things like only ever playing two moves. To force the model to explore beyond these local optimums, implement semi-random play, where a % of the time, a random move is played *instead of* the move the model thinks is best. By doing this, you are essentially forcing the model to explore other potential strategies that may go poorly, but ALSO may go well!